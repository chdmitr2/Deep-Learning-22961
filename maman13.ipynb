{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM784McRb2ys7xxSB1+hNis",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chdmitr2/Deep-Learning-22961/blob/main/maman13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maman 13\n",
        "\n",
        "Dmitriy Chudnovsky 324793900\n",
        "\n",
        "Question 1"
      ],
      "metadata": {
        "id": "c1kRriwB16V9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj_pW02MVXbW",
        "outputId": "7d20ce49-f471-4cc4-ddb0-2e33f0d566e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "=== One sample batch ===\n",
            "x shape: torch.Size([10, 10])\n",
            "y_class: tensor([0, 5, 4, 2, 6, 2, 7, 0, 7, 0])\n",
            "y_reg  : tensor([ 52., 160., 123.,  90., 170.,  87., 198.,  59., 232.,  60.])\n",
            "\n",
            "=== Decile Classification Results ===\n",
            "[Decile‑CLS] epoch 10  |  test-accuracy = 12.36%\n",
            "[Decile‑CLS] epoch 20  |  test-accuracy = 13.48%\n",
            "[Decile‑CLS] epoch 30  |  test-accuracy = 15.73%\n",
            "\n",
            "=== Standard Regression Results ===\n",
            "[Regression] epoch 10  |  R² on test = 0.216\n",
            "[Regression] epoch 20  |  R² on test = 0.278\n",
            "[Regression] epoch 30  |  R² on test = 0.317\n",
            "[Regression] epoch 40  |  R² on test = 0.325\n",
            "[Regression] epoch 50  |  R² on test = 0.335\n",
            "\n",
            "Explain the difference in the performance of the networks. Why is one better than the other?\n",
            " •  The regression model performs better because it naturally fits the task of predicting a continuous value,\n",
            "    while classification into deciles makes the problem harder and artificial — which harms the performance.\n",
            "    \n",
            "\n",
            "=== Percentile Classification Results ===\n",
            "[Percentile‑CLS] epoch 10  |  test-accuracy = 1.12%\n",
            "[Percentile‑CLS] epoch 20  |  test-accuracy = 1.12%\n",
            "[Percentile‑CLS] epoch 30  |  test-accuracy = 1.12%\n",
            "[Percentile‑CLS] epoch 40  |  test-accuracy = 1.12%\n",
            "[Percentile‑CLS] epoch 50  |  test-accuracy = 1.12%\n",
            "\n",
            "Deciles vs Percentiles? —\n",
            "    •  Deciles are generally preferred for most tasks \n",
            "            because they provide a better balance between accuracy and the number of categories.\n",
            "    •  Percentiles are useful if very specific categories are needed,\n",
            "            but with lower performance due to class imbalance.\n",
            "    \n",
            "\n",
            "=== Full Regression Results ===\n",
            "[Full‑Regression] epoch 10  |  R² on test = 0.215\n",
            "[Full‑Regression] epoch 20  |  R² on test = 0.338\n",
            "[Full‑Regression] epoch 30  |  R² on test = 0.375\n",
            "[Full‑Regression] epoch 40  |  R² on test = 0.385\n",
            "[Full‑Regression] epoch 50  |  R² on test = 0.379\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0.  Imports & reproducibility\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# Check if running on Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# ============================================================\n",
        "# 1.  Load local diabetes dataset (assumed already preprocessed)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        print(\"Drive might already be mounted. If needed, you can force remount.\")\n",
        "\n",
        "    # Path to your file inside Google Drive\n",
        "    data_path = \"/content/drive/MyDrive/Colab Notebooks/diabetes.csv\"\n",
        "\n",
        "else:\n",
        "    data_path = 'diabetes.csv'\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(data_path, delimiter=\"\\t\")\n",
        "\n",
        "# If \"Y\" is not the target column name, change it here\n",
        "if \"Y\" not in df.columns:\n",
        "    df.rename(columns={df.columns[-1]: \"Y\"}, inplace=True)\n",
        "\n",
        "# (a) rename features to f0, f1, ...\n",
        "feature_cols = [col for col in df.columns if col != \"Y\"]\n",
        "df.rename(columns={col: f\"f{i}\" for i, col in enumerate(feature_cols)}, inplace=True)\n",
        "\n",
        "#print(df[\"Y\"].dtype)  # Check data type\n",
        "#print(df[\"Y\"].head())  # Check first few values\n",
        "\n",
        "# (b) compute deciles (labels 1–10)\n",
        "df[\"Class_decile\"] = pd.qcut(df[\"Y\"], q=10, labels=False) + 1\n",
        "\n",
        "# ============================================================\n",
        "# 2.  Custom PyTorch Dataset\n",
        "# ------------------------------------------------------------\n",
        "# Define a custom Dataset class for the Diabetes dataset\n",
        "class DiabetesDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataframe: pd.DataFrame,\n",
        "        split: str = \"train\",\n",
        "        test_size: float = 0.2,\n",
        "        use_percentiles: bool = False,\n",
        "        random_state: int = 42,\n",
        "    ):\n",
        "        # Ensure 'split' parameter is either \"train\" or \"test\"\n",
        "        assert split in {\"train\", \"test\"}, \"`split` must be 'train' or 'test'\"\n",
        "\n",
        "        df = dataframe.copy()\n",
        "\n",
        "        # Choose whether to use 100 percentiles or 10 deciles for classification\n",
        "        if use_percentiles:\n",
        "            df[\"Class\"] = pd.qcut(df[\"Y\"], q=100, labels=False) + 1  # Divide Y into 100 quantile bins\n",
        "            self.n_classes = 100\n",
        "        else:\n",
        "            df[\"Class\"] = df[\"Class_decile\"]  # Use existing decile classes\n",
        "            self.n_classes = 10\n",
        "\n",
        "        # Split the dataset into training and testing sets\n",
        "        train_df, test_df = train_test_split(\n",
        "            df, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "        self.df = train_df if split == \"train\" else test_df\n",
        "\n",
        "        # Extract features (drop labels) and convert to PyTorch tensors\n",
        "        self.features = torch.tensor(\n",
        "            self.df.drop(columns=[\"Y\", \"Class_decile\", \"Class\"]).values,\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "\n",
        "        # Create label tensors for classification and regression\n",
        "        self.y_class = torch.tensor(self.df[\"Class\"].values - 1, dtype=torch.long)  # 0-based classes\n",
        "        self.y_reg = torch.tensor(self.df[\"Y\"].values, dtype=torch.float32)          # Regression targets\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sample (features and labels) by index\n",
        "        return {\n",
        "            \"x\": self.features[idx],\n",
        "            \"y_class\": self.y_class[idx],\n",
        "            \"y_reg\": self.y_reg[idx],\n",
        "        }\n",
        "\n",
        "# ============================================================\n",
        "# 3.  Model builders\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Build a classifier neural network\n",
        "def make_classifier(in_dim, n_classes):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_dim, 256), nn.ReLU(),    # First hidden layer with ReLU activation\n",
        "        nn.Linear(256, 128), nn.ReLU(),        # Second hidden layer with ReLU activation\n",
        "        nn.Linear(128, n_classes)              # Output layer for classification\n",
        "    )\n",
        "\n",
        "# Build a regressor neural network\n",
        "def make_regressor(in_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_dim, 64), nn.ReLU(),      # First hidden layer with ReLU\n",
        "        nn.Linear(64, 32), nn.ReLU(),           # Second hidden layer with ReLU\n",
        "        nn.Linear(32, 1)                        # Output single continuous value\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# 4.  Training / evaluation functions\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Train the model for one epoch\n",
        "def train_epoch(model, loader, criterion, optimizer, task=\"class\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(batch[\"x\"])  # Forward pass\n",
        "        if task == \"class\":\n",
        "            loss = criterion(preds, batch[\"y_class\"])  # Classification loss\n",
        "        else:\n",
        "            loss = criterion(preds.squeeze(1), batch[\"y_reg\"])  # Regression loss\n",
        "        loss.backward()    # Backward pass\n",
        "        optimizer.step()   # Update weights\n",
        "        total_loss += loss.item() * len(batch[\"x\"])\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# Evaluate the model without gradients\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, task=\"class\"):\n",
        "    model.eval()\n",
        "    if task == \"class\":\n",
        "        # For classification, compute accuracy\n",
        "        all_preds, all_true = [], []\n",
        "        for batch in loader:\n",
        "            logits = model(batch[\"x\"])\n",
        "            all_preds.append(logits.argmax(1).cpu().numpy())\n",
        "            all_true.append(batch[\"y_class\"].cpu().numpy())\n",
        "        return accuracy_score(np.concatenate(all_true), np.concatenate(all_preds))\n",
        "    else:\n",
        "        # For regression, compute R² score\n",
        "        preds, true = [], []\n",
        "        for batch in loader:\n",
        "            preds.append(model(batch[\"x\"]).squeeze(1).cpu().numpy())\n",
        "            true.append(batch[\"y_reg\"].cpu().numpy())\n",
        "        return r2_score(np.concatenate(true), np.concatenate(preds))\n",
        "\n",
        "# ============================================================\n",
        "# 5.  Create DataLoaders\n",
        "# ------------------------------------------------------------\n",
        "batch_size = 10\n",
        "\n",
        "# Create training and testing datasets\n",
        "train_ds = DiabetesDataset(df, split=\"train\", use_percentiles=False)\n",
        "test_ds = DiabetesDataset(df, split=\"test\", use_percentiles=False)\n",
        "\n",
        "# Create corresponding data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print a sample batch to verify\n",
        "print(\"\\n=== One sample batch ===\")\n",
        "sample = next(iter(train_loader))\n",
        "print(\"x shape:\", sample[\"x\"].shape)\n",
        "print(\"y_class:\", sample[\"y_class\"])\n",
        "print(\"y_reg  :\", sample[\"y_reg\"])\n",
        "\n",
        "# ============================================================\n",
        "# 6.  Train a classifier on decile classes\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Build the classifier model\n",
        "classifier = make_classifier(in_dim=train_ds.features.shape[1], n_classes=train_ds.n_classes)\n",
        "opt = torch.optim.Adam(classifier.parameters(), lr=1e-3)  # Optimizer\n",
        "criterion_cls = nn.CrossEntropyLoss()                     # Loss function for classification\n",
        "\n",
        "print(\"\\n=== Decile Classification Results ===\")\n",
        "for epoch in range(30):\n",
        "    train_epoch(classifier, train_loader, criterion_cls, opt, task=\"class\")\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        acc = evaluate(classifier, test_loader, task=\"class\")\n",
        "        print(f\"[Decile‑CLS] epoch {epoch+1:02d}  |  test-accuracy = {acc*100:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# 7.  Train a regressor on Y values\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Build the regressor model\n",
        "regressor = make_regressor(train_ds.features.shape[1])\n",
        "opt_r = torch.optim.Adam(regressor.parameters(), lr=1e-3)\n",
        "criterion_reg = nn.MSELoss()  # Mean Squared Error for regression\n",
        "\n",
        "print(\"\\n=== Standard Regression Results ===\")\n",
        "for epoch in range(50):\n",
        "    train_epoch(regressor, train_loader, criterion_reg, opt_r, task=\"reg\")\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        r2 = evaluate(regressor, test_loader, task=\"reg\")\n",
        "        print(f\"[Regression] epoch {epoch+1:02d}  |  R² on test = {r2:.3f}\")\n",
        "\n",
        "print(\n",
        "    \"\"\"\n",
        "Explain the difference in the performance of the networks. Why is one better than the other?\n",
        " •  The regression model performs better because it naturally fits the task of predicting a continuous value,\n",
        "    while classification into deciles makes the problem harder and artificial — which harms the performance.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 8.  (Optional) Train a classifier on 100 percentiles\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Create datasets and loaders using percentiles instead of deciles\n",
        "train_ds_pct = DiabetesDataset(df, split=\"train\", use_percentiles=True)\n",
        "test_ds_pct = DiabetesDataset(df, split=\"test\", use_percentiles=True)\n",
        "train_loader_pct = DataLoader(train_ds_pct, batch_size=batch_size, shuffle=True)\n",
        "test_loader_pct = DataLoader(test_ds_pct, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Build and train a classifier for 100 classes\n",
        "classifier_pct = make_classifier(train_ds_pct.features.shape[1], n_classes=100)\n",
        "opt_pct = torch.optim.Adam(classifier_pct.parameters(), lr=1e-3)\n",
        "criterion_pct = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n=== Percentile Classification Results ===\")\n",
        "for epoch in range(50):\n",
        "    train_epoch(classifier_pct, train_loader_pct, criterion_pct, opt_pct, task=\"class\")\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        acc_pct = evaluate(classifier_pct, test_loader_pct, task=\"class\")\n",
        "        print(f\"[Percentile‑CLS] epoch {epoch+1:02d}  |  test-accuracy = {acc_pct*100:.2f}%\")\n",
        "\n",
        "print(\n",
        "    \"\"\"\n",
        "Deciles vs Percentiles? —\n",
        "    •  Deciles are generally preferred for most tasks\n",
        "            because they provide a better balance between accuracy and the number of categories.\n",
        "    •  Percentiles are useful if very specific categories are needed,\n",
        "            but with lower performance due to class imbalance.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 9.  Full regression model (ignores Class)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Build a full regression model (without using the 'Class' column)\n",
        "regressor_full = make_regressor(in_dim=df.drop(columns=[\"Y\", \"Class_decile\"]).shape[1])\n",
        "opt_f = torch.optim.Adam(regressor_full.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"\\n=== Full Regression Results ===\")\n",
        "for epoch in range(50):\n",
        "    train_epoch(regressor_full, train_loader, criterion_reg, opt_f, task=\"reg\")\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        r2_full = evaluate(regressor_full, test_loader, task=\"reg\")\n",
        "        print(f\"[Full‑Regression] epoch {epoch+1:02d}  |  R² on test = {r2_full:.3f}\")\n",
        "\n"
      ]
    }
  ]
}