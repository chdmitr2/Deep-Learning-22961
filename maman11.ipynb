{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chdmitr2/Deep-Learning-22961/blob/main/maman11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maman 11**\n",
        "\n",
        "\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "\n",
        "**A. Broadcast one tensor A to the shape of another tensor B**\n",
        "\n",
        "\n",
        "We want a function broadcast_A_to_B(A, B) that attempts to broadcast A so that its shape matches B. We will do the following:\n",
        "\n",
        "1.   Check if broadcasting is possible using the rules described in Part B (i.e., compare dimensions from right to left).\n",
        "2.  If it is possible, insert (unsqueeze) dimensions of size 1 on the left of A as needed.\n",
        "3.  Foor each dimension where B has a size greater than 1, but A has a size of 1, we need to manually duplicate  A along that dimension.Since we cannot use .expand() or .repeat(), we will do this by concatenating multiple copies of A along that dimension. This way, A will match the shape of B\n"
      ],
      "metadata": {
        "id": "kisKcsIopU1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Helper function to replicate a tensor along a given dimension.\n",
        "# We cannot use x.repeat, so we manually cat copies of x.\n",
        "def replicate_dim(x, dim, times):\n",
        "    if times <= 1:\n",
        "        return x  # no replication needed if times=1\n",
        "    # We build a list of cloned tensors, then concatenate along `dim`.\n",
        "    clones = [x.clone() for _ in range(times)]\n",
        "    return torch.cat(clones, dim=dim)\n",
        "\n",
        "\n",
        "def broadcast_A_to_B(A, B):\n",
        "    \"\"\"\n",
        "    Broadcast tensor A to the shape of B, returning the broadcasted version\n",
        "    of A. Raises ValueError if it is impossible to broadcast.\n",
        "    \"\"\"\n",
        "    #  1. Check broadcastability (rely on a function from Part B, shown below).\n",
        "    can_broad, explanation = can_broadcast(A, B)\n",
        "    if not can_broad:\n",
        "        raise ValueError(\n",
        "            f\"Cannot broadcast A of shape {A.shape} to B of shape {B.shape}. Reason: {explanation}\"\n",
        "        )\n",
        "\n",
        "    #  2. Figure out how many dims to pad on the left\n",
        "    sA = list(A.shape)\n",
        "    sB = list(B.shape)\n",
        "    while len(sA) < len(sB):\n",
        "        sA.insert(0, 1)  # pad left\n",
        "    # We also want to unsqueeze in the actual tensor A\n",
        "    A_broad = A.clone()\n",
        "    while A_broad.dim() < len(sB):\n",
        "        A_broad = A_broad.unsqueeze(0)\n",
        "\n",
        "    #  3. Physically replicate A_broad in each dimension if needed\n",
        "    for dim in range(len(sB)):\n",
        "        sizeA = A_broad.size(dim)\n",
        "        sizeB = sB[dim]\n",
        "        if sizeA == 1 and sizeB > 1:\n",
        "            # replicate along this dimension\n",
        "            A_broad = replicate_dim(A_broad, dim, sizeB)\n",
        "        # if sizeA == sizeB, do nothing, it already matches\n",
        "\n",
        "    return A_broad\n"
      ],
      "metadata": {
        "id": "PCSdAa_br2Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Check if two tensors A and B can be broadcast together**\n",
        "\n",
        "We define a helper can_broadcast(A, B) that returns (True, explanation) if the two shapes are compatible under broadcasting rules, or (False, explanation) if they are not. In the explanation, we can also include the final broadcast shape if it is valid.\n",
        "\n",
        "Broadcast rules :\n",
        "\n",
        "1.   Compare the dimensions from the rightmost (last) going left.\n",
        "2.   At each dimension:\n",
        "    *   If the two sizes are equal, that dimension is fine.\n",
        "    *   Else if one of them is 1 and the other is > 1, that is also fine (the 1 can broadcast).\n",
        "    *   Otherwise, it is not possible to broadcast.\n",
        "    "
      ],
      "metadata": {
        "id": "iSP2c4b2szv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def can_broadcast(A, B):\n",
        "    \"\"\"\n",
        "    Checks if A and B can be broadcast together.\n",
        "    Returns (True, explanation) or (False, explanation).\n",
        "    \"\"\"\n",
        "    sA = list(A.shape)\n",
        "    sB = list(B.shape)\n",
        "    # Pad the shorter shape with 1 on the left\n",
        "    while len(sA) < len(sB):\n",
        "        sA.insert(0, 1)\n",
        "    while len(sB) < len(sA):\n",
        "        sB.insert(0, 1)\n",
        "\n",
        "    # Check dimension by dimension\n",
        "    for dimA, dimB in zip(sA, sB):\n",
        "        if not (dimA == dimB or dimA == 1 or dimB == 1):\n",
        "            return False, f\"Dimension mismatch ({dimA} vs {dimB})\"\n",
        "    # If we haven't returned False, they are broadcastable\n",
        "    # The final shape after broadcast:\n",
        "    final_shape = [max(dimA, dimB) for dimA, dimB in zip(sA, sB)]\n",
        "    return True, f\"Broadcast is possible. Final shape: {final_shape}\"\n"
      ],
      "metadata": {
        "id": "FJ8ORNYUthXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Joint broadcast of two tensors**\n",
        "\n",
        "Now we write a function that, given two tensors A and B, returns both of them in their “joint broadcasted” shape (the same shape for both). This is basically what torch.broadcast_tensors(A,B) does. We will:\n",
        "\n",
        "\n",
        "1.  Check if A and B can be broadcast (using our can_broadcast).\n",
        "2.  Compute the final broadcast shape.\n",
        "3.  Use the “broadcast_A_to_B” logic (or a similar approach) to broadcast each one to the final shape.\n",
        "4.  Return (A_broadcasted, B_broadcasted)."
      ],
      "metadata": {
        "id": "KJ9Lq9MPtqzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_broadcast_tensors(A, B):\n",
        "    \"\"\"\n",
        "    Returns a tuple of two new tensors (A_broad, B_broad) that both have\n",
        "    the same broadcasted shape, if possible.\n",
        "    \"\"\"\n",
        "    # 1. Check broadcast feasibility\n",
        "    can_broad, explanation = can_broadcast(A, B)\n",
        "    if not can_broad:\n",
        "        raise ValueError(f\"Cannot broadcast shapes {A.shape} and {B.shape}. {explanation}\")\n",
        "\n",
        "    # 2. Derive the final broadcast shape from the explanation text or by re-computing\n",
        "    sA = list(A.shape)\n",
        "    sB = list(B.shape)\n",
        "    while len(sA) < len(sB):\n",
        "        sA.insert(0, 1)\n",
        "    while len(sB) < len(sA):\n",
        "        sB.insert(0, 1)\n",
        "\n",
        "    # final broadcast shape = elementwise max\n",
        "    final_shape = [max(a, b) for a, b in zip(sA, sB)]\n",
        "\n",
        "    # 3. Broadcast each separately to final_shape\n",
        "    A_out = broadcast_to_shape(A, final_shape)  # see below\n",
        "    B_out = broadcast_to_shape(B, final_shape)\n",
        "\n",
        "    return A_out, B_out\n",
        "\n",
        "\n",
        "def broadcast_to_shape(X, desired_shape):\n",
        "    \"\"\"\n",
        "    Helper that broadcasts X to 'desired_shape' using the same replicate approach.\n",
        "    \"\"\"\n",
        "    # 1. Check if X can be broadcast to desired_shape\n",
        "    #    We can do a quick shape-compatibility check:\n",
        "    sX = list(X.shape)\n",
        "    while len(sX) < len(desired_shape):\n",
        "        sX.insert(0, 1)\n",
        "\n",
        "    for (dimX, dimWant) in zip(reversed(sX), reversed(desired_shape)):\n",
        "        # compare from right to left\n",
        "        if not (dimX == dimWant or dimX == 1 or dimWant == 1):\n",
        "            raise ValueError(f\"Shape mismatch: cannot match {X.shape} to {desired_shape}.\")\n",
        "\n",
        "    # 2. Actually replicate data as needed\n",
        "    X_broad = X.clone()\n",
        "    # unsqueeze if needed\n",
        "    while X_broad.dim() < len(desired_shape):\n",
        "        X_broad = X_broad.unsqueeze(0)\n",
        "\n",
        "    # Now replicate in each dimension\n",
        "    for dim in range(len(desired_shape)):\n",
        "        sizeX = X_broad.size(dim)\n",
        "        sizeD = desired_shape[dim]\n",
        "        if sizeX == 1 and sizeD > 1:\n",
        "            X_broad = replicate_dim(X_broad, dim, sizeD)\n",
        "        # If they are equal, do nothing\n",
        "\n",
        "    return X_broad\n"
      ],
      "metadata": {
        "id": "NF40mBfswsak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Demonstration & Comparison with built-in PyTorch**\n",
        "\n",
        "Below we create some example tensors of various shapes and compare the results of our “manual” broadcasting versus PyTorchs built-in broadcasting (e.g. by doing an elementwise operation).We can use normal PyTorch broadcasting in typical expressions (like A + B) just to check that the final shapes/values match."
      ],
      "metadata": {
        "id": "m1sNlz58w5rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define some example shapes:\n",
        "examples = [\n",
        "    (torch.rand(3),      torch.rand(3)),\n",
        "    (torch.rand(1,5),    torch.rand(3,1,5)),\n",
        "    (torch.rand(2,1,4),  torch.rand(2,3,4)),\n",
        "    (torch.rand(1,1),    torch.rand(2,3)),\n",
        "]\n",
        "\n",
        "for i, (A, B) in enumerate(examples, 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(\"A.shape =\", A.shape, \"B.shape =\", B.shape)\n",
        "\n",
        "    # A. Using our function from Part A: broadcast A to shape of B\n",
        "    try:\n",
        "        A_to_B = broadcast_A_to_B(A, B)\n",
        "        print(\"Shape of broadcast_A_to_B(A,B) =\", A_to_B.shape)\n",
        "    except ValueError as e:\n",
        "        print(\"Error in broadcast_A_to_B:\", e)\n",
        "\n",
        "    # B. Check broadcast\n",
        "    can_broad, expl = can_broadcast(A, B)\n",
        "    print(\"can_broadcast(A,B) =\", can_broad, \"|\", expl)\n",
        "\n",
        "    # C. Joint broadcast\n",
        "    try:\n",
        "        A_broad, B_broad = my_broadcast_tensors(A, B)\n",
        "        print(\"Shapes from my_broadcast_tensors: A_broad:\", A_broad.shape,\n",
        "              \"B_broad:\", B_broad.shape)\n",
        "    except ValueError as e:\n",
        "        print(\"Error in my_broadcast_tensors:\", e)\n",
        "\n",
        "    # Compare with PyTorch's normal broadcasting by doing an elementwise op\n",
        "    try:\n",
        "        C_pytorch = A + B  # triggers normal PyTorch broadcasting internally\n",
        "        print(\"PyTorch result shape (A + B):\", C_pytorch.shape)\n",
        "        # We can check that A_broad + B_broad matches this as well:\n",
        "        check = (A_broad + B_broad).shape == C_pytorch.shape\n",
        "        print(\"Check shapes match our broadcast vs PyTorch:\", check)\n",
        "    except RuntimeError as e:\n",
        "        print(\"PyTorch broadcasting error (which should match ours):\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToNmoY5nxLVg",
        "outputId": "d8907232-fa0a-466d-85fe-ca63c75285f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1 ---\n",
            "A.shape = torch.Size([3]) B.shape = torch.Size([3])\n",
            "Shape of broadcast_A_to_B(A,B) = torch.Size([3])\n",
            "can_broadcast(A,B) = True | Broadcast is possible. Final shape: [3]\n",
            "Shapes from my_broadcast_tensors: A_broad: torch.Size([3]) B_broad: torch.Size([3])\n",
            "PyTorch result shape (A + B): torch.Size([3])\n",
            "Check shapes match our broadcast vs PyTorch: True\n",
            "\n",
            "--- Example 2 ---\n",
            "A.shape = torch.Size([1, 5]) B.shape = torch.Size([3, 1, 5])\n",
            "Shape of broadcast_A_to_B(A,B) = torch.Size([3, 1, 5])\n",
            "can_broadcast(A,B) = True | Broadcast is possible. Final shape: [3, 1, 5]\n",
            "Shapes from my_broadcast_tensors: A_broad: torch.Size([3, 1, 5]) B_broad: torch.Size([3, 1, 5])\n",
            "PyTorch result shape (A + B): torch.Size([3, 1, 5])\n",
            "Check shapes match our broadcast vs PyTorch: True\n",
            "\n",
            "--- Example 3 ---\n",
            "A.shape = torch.Size([2, 1, 4]) B.shape = torch.Size([2, 3, 4])\n",
            "Shape of broadcast_A_to_B(A,B) = torch.Size([2, 3, 4])\n",
            "can_broadcast(A,B) = True | Broadcast is possible. Final shape: [2, 3, 4]\n",
            "Shapes from my_broadcast_tensors: A_broad: torch.Size([2, 3, 4]) B_broad: torch.Size([2, 3, 4])\n",
            "PyTorch result shape (A + B): torch.Size([2, 3, 4])\n",
            "Check shapes match our broadcast vs PyTorch: True\n",
            "\n",
            "--- Example 4 ---\n",
            "A.shape = torch.Size([1, 1]) B.shape = torch.Size([2, 3])\n",
            "Shape of broadcast_A_to_B(A,B) = torch.Size([2, 3])\n",
            "can_broadcast(A,B) = True | Broadcast is possible. Final shape: [2, 3]\n",
            "Shapes from my_broadcast_tensors: A_broad: torch.Size([2, 3]) B_broad: torch.Size([2, 3])\n",
            "PyTorch result shape (A + B): torch.Size([2, 3])\n",
            "Check shapes match our broadcast vs PyTorch: True\n"
          ]
        }
      ]
    }
  ]
}